{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['text']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Natural language processing (NLP) Is A SuBfield Of Computer scIence, inFormation eNgineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data!?!!....'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'natural language processing (nlp) is a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data!?!!....'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 大小写转换\n",
    "lower = text.lower()\n",
    "lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'natural language processing nlp is a subfield of computer science information engineering and artificial intelligence concerned with the interactions between computers and human natural languages in particular how to program computers to process and analyze large amounts of natural language data'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标点符号的处理\n",
    "#string.punctuation中包含英文的标点，我们将其放在待去除变量remove中\n",
    "#函数需要三个参数，前两个表示字符的映射，我们是不需要的。\n",
    "remove = str.maketrans('','',string.punctuation) \n",
    "without_punctuation = lower.translate(remove)\n",
    "without_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = without_punctuation.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_path = 'D:/anydata/data/stopword.txt'\n",
    "stopwords_list = [line.rstrip() for line in open(stopwords_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'nlp',\n",
       " 'subfield',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'information',\n",
       " 'engineering',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'concerned',\n",
       " 'interactions',\n",
       " 'computers',\n",
       " 'human',\n",
       " 'natural',\n",
       " 'languages',\n",
       " 'particular',\n",
       " 'program',\n",
       " 'computers',\n",
       " 'process',\n",
       " 'analyze',\n",
       " 'amounts',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'data']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 去除停用词\n",
    "without_stopwords = [w for w in tokens if not w in stopwords_list]\n",
    "without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process', 'nlp', 'subfield', 'comput', 'scienc', 'inform', 'engin', 'artifici', 'intellig', 'concern', 'interact', 'comput', 'human', 'natur', 'languag', 'particular', 'program', 'comput', 'process', 'analyz', 'amount', 'natur', 'languag', 'data']\n"
     ]
    }
   ],
   "source": [
    "import nltk.stem\n",
    "s = nltk.stem.SnowballStemmer('english')  #参数是选择的语言\n",
    "cleaned_text = [s.stem(ws) for ws in without_stopwords]\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natur',\n",
       " 'languag',\n",
       " 'process',\n",
       " 'nlp',\n",
       " 'subfield',\n",
       " 'comput',\n",
       " 'scienc',\n",
       " 'inform',\n",
       " 'engin',\n",
       " 'artifici',\n",
       " 'intellig',\n",
       " 'concern',\n",
       " 'interact',\n",
       " 'comput',\n",
       " 'human',\n",
       " 'natur',\n",
       " 'languag',\n",
       " 'particular',\n",
       " 'program',\n",
       " 'comput',\n",
       " 'process',\n",
       " 'analyz',\n",
       " 'amount',\n",
       " 'natur',\n",
       " 'languag',\n",
       " 'data']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "[wnl.lemmatize(ws) for ws in cleaned_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词云"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "import sys\n",
    "import os\n",
    "from pprint import pprint\n",
    "import codecs\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = sys.path[0] + os.sep\n",
    "\n",
    "def wc_from_text(str, fn):\n",
    "\t'''根据字符串进行统计，并生成词云图'''\n",
    "    wc = WordCloud(\n",
    "        background_color=\"white\",  # 设置背景为白色，默认为黑色\n",
    "        width = 1500,  # 设置图片的宽度\n",
    "        height= 960,  # 设置图片的高度\n",
    "        margin= 10  # 设置图片的边缘\n",
    "    ).generate(s)\n",
    "    plt.imshow(wc)  # 绘制图片\n",
    "    plt.axis(\"off\")  # 消除坐标轴\n",
    "    plt.show()  # 展示图片\n",
    "    # wc.to_file(path + fn)  # 保存图片\n",
    "\n",
    "def wc_from_word_count(word_count, fp):\n",
    "\t'''根据词频字典生成词云图'''\n",
    "    wc = WordCloud(\n",
    "        max_words=500,  # 最多显示词数\n",
    "        # max_font_size=100,  # 字体最大值\n",
    "        background_color=\"white\",  # 设置背景为白色，默认为黑色\n",
    "        width = 1500,  # 设置图片的宽度\n",
    "        height= 960,  # 设置图片的高度\n",
    "        margin= 10  # 设置图片的边缘\n",
    "    )\n",
    "    wc.generate_from_frequencies(word_count)  # 从字典生成词云\n",
    "    plt.imshow(wc)  # 显示词云\n",
    "    plt.axis('off')  # 关闭坐标轴\n",
    "    plt.show()  # 显示图像\n",
    "    wc.to_file(fp)  # 保存图片\n",
    "\n",
    "def generate_dict_from_file(fp):\n",
    "    with codecs.open(fp, 'r', 'utf-8') as source_file:\n",
    "        for line in source_file:\n",
    "            dic = json.loads(line)\n",
    "            yield dic\n",
    "\n",
    "def main(data_fp, pic_fp):\n",
    "    word_count = defaultdict(lambda: 0)\n",
    "    for dic in generate_dict_from_file(data_fp):\n",
    "        words = dic['content'].split(' ')\n",
    "        for word in words:\n",
    "        \tword_count[word] += 1\n",
    "    with codecs.open(path + 'word_count.json', 'w', 'utf-8') as f:\n",
    "        json.dump(word_count, f, ensure_ascii=False)\n",
    "    wc_from_word_count(word_count, pic_fp)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    s = 'access restored ban remains blocked government order accessible aid proxy provider telecom restored access celebrating government revoked censorship order newsroom waiting appeal court lawsuit government allowed constitution reporting stringer spread dedication journalism critical reporting brought outlet respect recognition landed blacklist authoritarian regime dominate permanently blocked severe intolerance critical journalism authority deny domestic access occasional basis regional outlet sensitive issue incident hard technical glitch deliberately blocked access depending covered government corruption human abuse social discontent policy freedom protested blocked violent conflict ethnic resident authority imposed permanent ban parliament resolution lawmaker addressed conflict recommended action government resolution reason obtaining court order law shutting outlet introduce measure domain space resolution authority'\n",
    "    # wc_from_text(s, 'wc1.jpg')\n",
    "    # word_count = Counter(s.split(' '))\n",
    "    # wc_from_word_count(word_count, 'wc2.jpg')\n",
    "    data_fp = path + 'result.json'\n",
    "    pic_fp = path + 'word_cloud_uz.jpg'\n",
    "    main(data_fp, pic_fp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
